{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Lineal Simple. Un ejemplo minimalista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar las librerías relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Para graficar en 3-D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar datos al azar para entrenar al modelo\n",
    "\n",
    "Se trabajará con dos variables de entrada, las x1 y x2 en los ejemplos vistos en clase. Se generan al azar a partir de una distribución uniforme.\n",
    "\n",
    "Se creará una matriz con estas dos variables.  La matriz X del modelo lineal y = x * w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Por facilidad, se declara una variable para indicar el tamaño del conjunto \n",
    "#      de datos de entrenamiento. Puede probarse con 100,000 o 1,000,000 pero hay \n",
    "#      que tener cuidado ya que con tantas observaciones puede que la máquina se quede!\n",
    "\n",
    "observaciones = 1000\n",
    "\n",
    "x1 = np.random.uniform(low = -10, high = 10, size = (observaciones,1))\n",
    "x2 = np.random.uniform(-10, 10, (observaciones,1))\n",
    "\n",
    "X = np.column_stack((x1,x2))\n",
    "\n",
    "# Verificar la forma de la matriz \n",
    "# Debiera ser n x k, donde n es el número de observaciones, y k es el número de variables.\n",
    "\n",
    "print (X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar las metas a las que debemos apuntar\n",
    "\n",
    "Para el modelo se usará la función f(x1, x2) = 2 * x1 - 3 * x2 + 5 + <ruido pequeño>.  El ruido es para hacerlo más realista.\n",
    "\n",
    "Se utiliza la metodología de ML, y al finalizar se determinará si el algoritmo la ha aprendido.  \n",
    "\n",
    "Al utilizar esta función, se espera que la red neuronal genere los pesos w1 = 2, w2 = -3 y el sesgo b = 5.  Si no se logra, es que algo se ha hecho mal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ruido = np.random.uniform(-1, 1, (observaciones,1))\n",
    "\n",
    "metas = 2 * x1 - 3 * x2 + 5 + ruido\n",
    "\n",
    "# Verificar las dimensiones. Deben ser n x m, donde m es el número de variables de salida.\n",
    "print (metas.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficar los datos a usar para el entrenamiento\n",
    "\n",
    "La idea es ver que haya una fuerte tendencia que el modelo debe aprender a reproducir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x1.shape)\n",
    "print(x2.shape)\n",
    "print(metas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1N = x1.reshape(observaciones,)\n",
    "x2N = x2.reshape(observaciones,)\n",
    "metasN = metas.reshape(observaciones,)\n",
    "\n",
    "fig = px.scatter_3d(x = x1N, y = x2N, z = metasN)\n",
    "\n",
    "fig.update_layout(\n",
    "    width = 500,\n",
    "    height = 500,)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inicializar variables\n",
    "\n",
    "Se inicializan los pesos y sesgos, al azar, dentro de un rango inicial pequeño.  Es posible \"jugar\" con este valor pero no es recomendable ya que el uso de rangos iniciales altos inhibe el aprendizaje por parte del algoritmo\n",
    "\n",
    "Los pesos son de dimensiones k x m, donde k es el numero de variables de entrada y m es el número de variables de salida.  \n",
    "\n",
    "Como solo hay una salida, el sesgo es de tamaño 1, y es un escalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "rango_inicial = 0.1     #  valor máximo para los pesos y sesgos iniciales\n",
    "\n",
    "pesos = np.random.uniform(low = -rango_inicial, high = rango_inicial, size = (2, 1))\n",
    "\n",
    "sesgos = np.random.uniform(low = -rango_inicial, high = rango_inicial, size = 1)\n",
    "\n",
    "#Ver cómo fueron inicializados.\n",
    "print (pesos)\n",
    "print (sesgos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pesos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asignar la tasa de aprendizaje (Eta)\n",
    "\n",
    "Se asigna una tasa de aprendizaje pequeña.  Para este ejemplo funciona bien 0.02.  Vale la pena \"jugar\" con este valor para ver los resultados de hacerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "eta = 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenar el modelo\n",
    "\n",
    "Se utilizará un valor de 100 para iterar el modelo con el conjunto de datos de entrenamiento.  Ese valor funciona bastante bien con la tasa de aprendizaje de 0.02.  Cómo saber el número adecuado de iteraciones es algo que se verá en futuras sesiones, pero generalmente una tasa de aprendizaje baja requiere de más iteraciones que una más alta.  Sin embargo hay que tener en mente que una tasa de aprendizaje alta puede causar que la pérdida \"Loss\" diverja a infinito, en vez de converger a cero (0)\n",
    "\n",
    "Puesto que esta es una regresión, se usará la función de pérdida L2-norm (dividido por 2, para ser consistente con la clase).  Es más, también se dividirá por el número de observaciones para obtener un promedio de pérdida por observación.  Se discutió en clase sobre la libertad de modificar esta función una vez no se pierda la característica de ser más baja para los resultados mejores, y vice versa.\n",
    "\n",
    "Se mostrará la función de pérdida (loss) en cada iteración, para ver si está decreciendo como se desea.\n",
    "\n",
    "Otro pequeño truco es escalar las deltas de la misma manera que se hizo con la función de pérdida.  De esta forma la tasa de aprendizaje es independiente del número de observaciones.  De nuevo esto no cambia el principio, solo hace más fácil la selección de una tasa única de aprendizaje. \n",
    "\n",
    "Finalmente se aplica la regla de actualización del decenso de gradiente.\n",
    "\n",
    "Ojo!  los pesos son de dimensión 2 X 1, la tasa de aprendizaje es 1 X 1 (escalar), las entradas son 1000 X 2, y las deltas escaladas son 1000 X 1.  Es necesario obtener la transpuesta de las entradas para que no hayan problemas de dimensión en las operaciones. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range (100):\n",
    "    \n",
    "    # Esta es la ecuacion del modelo lineal: y = xw + b \n",
    "    y = np.dot(X, pesos) + sesgos\n",
    "    \n",
    "    # Las deltas son las diferencias entre las salidas y las metas (targets)\n",
    "    # deltas es un vector 1000 x 1\n",
    "    deltas = y - metas\n",
    "        \n",
    "    perdida = np.sum(deltas ** 2) / 2 / observaciones\n",
    "    \n",
    "    print(perdida)\n",
    "    \n",
    "    deltas_escaladas = deltas / observaciones\n",
    "      \n",
    "    pesos = pesos - eta * np.dot(X.T, deltas_escaladas)\n",
    "    sesgos = sesgos - eta * np.sum(deltas_escaladas)\n",
    "    \n",
    "    # Los pesos son actualizados en forma de algebra lineal(una matriz menos otra)\n",
    "    # Sin embargo, los sesgos en este caso son solo un número (solo se calcula una salida), \n",
    "    #       es necesario transformar las deltas a un escalar.      \n",
    "    # Ambas líneas son consistentes con la metodología de decenso de gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desplegar los pesos y el sesgo para ver si funcionaron correctamente.\n",
    "\n",
    "Por el diseño de los datos, los pesos finales deben ser 2 y -3, y el sesgo: 5\n",
    "\n",
    "**NOTA:**  Si aún no están los valores correctos, puede que aún estén convergiendo y sea necesario iterar más veces.  Para esto solo se requiere ejecutar la celda anterior cuantas veces sea requerido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(pesos, sesgos)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficar las últimas salidas vrs las metas (targets)\n",
    "\n",
    "Como son los últimos valores, luego del entrenamiento, representan la exactitut del modelo final de.  Entre más cercana esté esta gráfica a una línea de 45 grados, más cercanas están las salidas y metas.\n",
    "\n",
    "Como este ejemplo es pequeño, es posible hacerlo, en los problemas que se veran posteriormente en el curso, esto ya no sería posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yN = y.reshape(observaciones,)\n",
    "metasN = metas.reshape(observaciones,)\n",
    "fig = px.scatter(x = yN, y =  metasN)\n",
    "\n",
    "fig.update_layout(\n",
    "    width = 400,\n",
    "    height = 400,)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
